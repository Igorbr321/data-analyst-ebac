{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJqp9AANOCtf"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/andre-marcos-perez/ebac-course-utils/main/media/logo/newebac_logo_black_half.png\" alt=\"ebac-logo\">\n",
        "\n",
        "---\n",
        "\n",
        "# **Módulo** | Big Data II - Armazenamento\n",
        "Caderno de **Aula**<br> \n",
        "Professor [André Perez](https://www.linkedin.com/in/andremarcosperez/)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9jDtUbDOE1-"
      },
      "source": [
        "# **Tópicos**\n",
        "\n",
        "<ol type=\"1\">\n",
        "  <li>Introdução;</li>\n",
        "  <li>Orientação a coluna;</li>\n",
        "  <li>Particionamento.</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmoHgt-lwkpD"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GABI6OW8OfQ2"
      },
      "source": [
        "# **Aulas**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Rig-cljwJdi"
      },
      "source": [
        "## 1\\. Introdução"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay4UkojwGskg"
      },
      "source": [
        "### **1.1. Armazenamento distribuído** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUonSt7kxZyj"
      },
      "source": [
        "A escala horizontal de recursos faz com que os dados sejam armazenados em arquivos (`csv`, `txt`, `parquet`, etc.), \"quebrados\" em blocos (128 MB geralmente) e **distribuídos** e **replicados** (três vezes geralmente) entre os nós do cluster. O gerenciador de *cluster* mantem um mapa da distribuição dos blocos. Esta característica de sistemas distribuídos é abstraída dos usuários comuns de um *cluster*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YhdREnYxZyj"
      },
      "source": [
        "### **1.2. Orientação a coluna** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUDMrBks-1MF"
      },
      "source": [
        "Tradicionalmente, os sistemas de armazenamento de dados (arquivos, bases de dados, etc.) trabalham com **orientação a linha**, ou seja, para acessar o valor de uma coluna, primeiro encontra-se sua linha. Como exemplo, imagine uma base de dados de vendas de jogos eletrônicos com a seguinte estrutura."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdOkyenE0hNp"
      },
      "outputs": [],
      "source": [
        "%%writefile jogos.csv\n",
        "\"id\",\"nome\",\"plataforma\",\"ano_lancamento\",\"total_vendas_mm\"\n",
        "100,\"Final Fantasy VII\",\"PSX\",1997,12.3\n",
        "101,\"Final Fantasy VIII\",\"PSX\",1999,9.6\n",
        "102,\"Final Fantasy IX\",\"PSX\",2000,5.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ1ZhOOh2ESI"
      },
      "source": [
        "A consulta SQL abaixo primeiro encontraria a linha com o `id` igual a 102 para então acessar o valor de 5.5 na coluna `total_vendas_mm`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BrLhw1e2g6f"
      },
      "source": [
        "```sql\n",
        "SELECT total_vendas_mm FROM jogos WHERE id = 102 \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNNvDoQo2oNs"
      },
      "source": [
        "Portanto, no formato **orientado a linha**, consultas com métricas de **agregação** faz com que o acesso ao dado da coluna a ser agregada também seja extraído linha a linha. Como exemplo, a consulta SQL abaixo seria equivalente ao código Python também abaixo:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4TM0YGE3aqf"
      },
      "source": [
        "```sql\n",
        "SELECT SUM(total_vendas_mm) FROM jogos\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jOFkVLe5Uhh"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from functools import reduce\n",
        "\n",
        "vals = []\n",
        "lines = None\n",
        "\n",
        "with open('jogos.csv', mode='r') as fp:\n",
        "  lines = csv.reader(fp)\n",
        "  next(lines, None)\n",
        "  for line in lines:\n",
        "    vals.append(float(line[4]))\n",
        "\n",
        "sum_vals = reduce(lambda x, y: x + y, vals)\n",
        "\n",
        "print(sum_vals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIJJGU5V6VYN"
      },
      "source": [
        "Em geral, para o volume das bases de dados modernas, essa abordagem é inviável. Para lidar com essa situação, foram criados tipos de arquivos (`Apache Parquet`), bases de dados (`Apache HBase`) e estruturas de dados (`Apache Arrow`) **orientados a colunas**, onde os dados são \"pivotados\", ou seja, dados de uma mesma coluna são organizados como se estivessem em mesma linha. Portanto, a mesma consulta SQL (replicada abaixo) realizada em um sistema **orientado a colunas** executaria muito mais rápido."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef5X1baT98tq"
      },
      "source": [
        "```sql\n",
        "SELECT SUM(total_vendas_mm) FROM jogos\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnGd-HIuG9NI"
      },
      "source": [
        "> Sistemas **orientados a colunas** são ideias para **agregações** (base de cargas analíticas)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJd9OU2599pD"
      },
      "source": [
        "Vamos explorar o `Apache Parquet` e o `Apache Arrow` na aula 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnJI1bszxZyk"
      },
      "source": [
        "### **1.3. Particionamento** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OGjIWy9xZyk"
      },
      "source": [
        "Dados são armazenados em uma estrutura de pastas, conhecidas como partições, de tal forma que apenas os dados nas partições de interesse são acessados. Como exemplo, imagine uma base de dados de entregas de um aplicativo de entrega de comidas com a seguinte estrutura:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uj2xGZUsglc8"
      },
      "outputs": [],
      "source": [
        "%%writefile entrega.csv\n",
        "\"id_entrega\",\"id_resturante\",\"cidade\",\"estado\",\"data\"\n",
        "100,24,\"Piracicaba\",\"SP\",2022-01-01\n",
        "101,25,\"Piracicaba\",\"SP\",2022-01-01\n",
        "102,26,\"Campinas\",\"SP\",2022-01-02\n",
        "103,27,\"Florianopolis\",\"SC\",2022-01-02\n",
        "104,28,\"Florianopolis\",\"SC\",2022-01-03"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlIrLCdZl4DE"
      },
      "source": [
        "Particionar a base por `data` geraria três partições: `2022-01-01`, `2022-01-02` e `2022-01-03`. O efeito no sistema de arquivos seria equivalente ao resultado da execução do código abaixo. Note que a operação \"move\" a coluna de particionamento `data` dos arquivos `csv` para a estrutura de pastas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iih2uMpLydG6"
      },
      "source": [
        "> **Nota**: Não se preocupe em entender o código `bash` abaixo, os pacotes Python de interesse abstraem essa complexidade. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9LR8YrKmU2j"
      },
      "outputs": [],
      "source": [
        "!mkdir ./entregas\n",
        "!mkdir ./entregas/data=2022-01-01\n",
        "!mkdir ./entregas/data=2022-01-02\n",
        "!mkdir ./entregas/data=2022-01-03\n",
        "\n",
        "!echo \"id_entrega,id_resturante,cidade,estado\" >> ./entregas/data=2022-01-01/entregas_part1.csv\n",
        "!echo \"100,24,Piracicaba,SP\" >> ./entregas/data=2022-01-01/entregas_part1.csv\n",
        "!echo \"101,25,Piracicaba,SP\" >> ./entregas/data=2022-01-01/entregas_part1.csv\n",
        "\n",
        "!echo \"id_entrega,id_resturante,cidade,estado\" >> ./entregas/data=2022-01-02/entregas_part2.csv\n",
        "!echo \"102,26,Campinas,SP\" >> ./entregas/data=2022-01-02/entregas_part2.csv\n",
        "!echo \"103,27,Florianopolis,SC\" >> ./entregas/data=2022-01-02/entregas_part2.csv\n",
        "\n",
        "!echo \"id_entrega,id_resturante,cidade,estado\" >> ./entregas/data=2022-01-03/entregas_part3.csv\n",
        "!echo \"104,28,Florianopolis,SC\" >> ./entregas/data=2022-01-03/entregas_part3.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp4GpbH6pgw_"
      },
      "source": [
        "Logo, a consulta SQL abaixo retornaria apenas o conteúdo do arquivo `csv` da pasta `data=2022-01-02`, reduzindo assim o tráfego de dados pela rede de computadores que conecta os nós do clusters, o que se traduz em velocidade na consulta (e redução do preço caso esteja usando serviços de computação em nuvem como o AWS Athena)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg5AWE2YpqLi"
      },
      "source": [
        "```sql\n",
        "SELECT * FROM entregas WHERE \"data\" = DATE '2022-01-02' \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhIXyNkerP2H"
      },
      "source": [
        "É possível o particionamento em multinível também. Como exemplo, um particionamento por `data` e `estado` geraria uma estrutura de pastas equivalente ao resultado da execução do código abaixo. Note que desta vez tanto a coluna `data` como a coluna `estado` são \"movidas\" dos arquivos `csv` para a estrutua de pastas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUlJdXR8yLtI"
      },
      "source": [
        "> **Nota**: Não se preocupe em entender o código `bash` abaixo, os pacotes Python de interesse abstraem essa complexidade. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tod53ncsHGD"
      },
      "outputs": [],
      "source": [
        "!mkdir ./entregas_multi\n",
        "!mkdir ./entregas_multi/data=2022-01-01\n",
        "!mkdir ./entregas_multi/data=2022-01-01/estado=SP\n",
        "!mkdir ./entregas_multi/data=2022-01-02\n",
        "!mkdir ./entregas_multi/data=2022-01-02/estado=SP\n",
        "!mkdir ./entregas_multi/data=2022-01-02/estado=SC\n",
        "!mkdir ./entregas_multi/data=2022-01-03\n",
        "!mkdir ./entregas_multi/data=2022-01-03/estado=SC\n",
        "\n",
        "!echo \"id_entrega,id_resturante,cidade\" >> ./entregas_multi/data=2022-01-01/estado=SP/entregas_part1.csv\n",
        "!echo \"100,24,Piracicaba\" >> ./entregas_multi/data=2022-01-01/estado=SP/entregas_part1.csv\n",
        "!echo \"101,25,Piracicaba\" >> ./entregas_multi/data=2022-01-01/estado=SP/entregas_part1.csv\n",
        "\n",
        "!echo \"id_entrega,id_resturante,cidade\" >> ./entregas_multi/data=2022-01-02/estado=SP/entregas_part2.csv\n",
        "!echo \"102,26,Campinas\" >> ./entregas_multi/data=2022-01-02/estado=SP/entregas_part2.csv\n",
        "\n",
        "!echo \"id_entrega,id_resturante,cidade\" >> ./entregas_multi/data=2022-01-02/estado=SC/entregas_part3.csv\n",
        "!echo \"103,27,Florianopolis\" >> ./entregas_multi/data=2022-01-02/estado=SC/entregas_part3.csv\n",
        "\n",
        "!echo \"id_entrega,id_resturante,cidade\" >> ./entregas_multi/data=2022-01-03/estado=SC/entregas_part4.csv\n",
        "!echo \"104,28,Florianopolis\" >> ./entregas_multi/data=2022-01-03/estado=SC/entregas_part4.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mjl_zcCtE1f"
      },
      "source": [
        "Logo, a consulta SQL abaixo retornaria apenas o conteúdo do arquivo `csv` da pasta `estado=SC` dentro da pasta `data=2022-01-02`, diminuindo ainda mais o trafego de dados na rede de computadores do cluster mas aumentando o processameno necessário para \"varrer\" as pastas do sistema de arquivos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQrGILjZtKlB"
      },
      "source": [
        "```sql\n",
        "SELECT * FROM entregas WHERE \"data\" = DATE '2022-01-02' AND estado = 'SC'\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOym8jmht7V5"
      },
      "source": [
        "> Portanto, as escolha das colunas de partição é uma escolha de compromisso entre **tráfego** (custo) e **velocidade** (processamento)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH9FPa9zuXEG"
      },
      "source": [
        "Colunas como `id_entrega` ou ainda `id_resturante` seriam escolhas infelizes pois gerariam muitas partições com arquivos com poucas linhas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvf9r7KBuw4w"
      },
      "source": [
        "> **Dica**: Em geral, colunas de tempo no formato YYYY-MM-DD é uma escolha razoável."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiaOtH9lvCwW"
      },
      "source": [
        "> **Dica**: Se possível, demais colunas de partição (além das que remetem ao tempo) devem ser selecionadas de acordo com o padrão de consulta dos dados. Contudo, é muito improvável prever esse tipo de padrão com uma boa assertividade."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nj8pCF_jE0Nd"
      },
      "source": [
        "Vamos explorar técnicas de particionamento (e orientação a coluna) na nuvem da AWS com o AWS S3 e AWS Athena na aula 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvJTE2FpaGeU"
      },
      "source": [
        "## 2\\. Orientação a coluna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISYXZbomlb1x"
      },
      "source": [
        "Para obsevar os benefícios que a orientação a coluna trás para o armazenamento de grandes volumes de dados, vamos explorar duas tecnologias orientadas a colunas: o formato de arquivo `Apache Parquet` (disco) e a estrutura de dados `Apache Arrow` (memória). Vamos também compará-las com seus pares orientados a linha, como arquivos do tipo `csv` e o pacote Python Pandas.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MlzEchdu0cP"
      },
      "source": [
        "Como exemplo, vamos utilizar os dados de crime da cidade de Chicago, Estados Unidos da América, em 2014. Os dados estão armazenados em um arquivo no formato `csv` de aproimadamente 50MB e foram extraídos do Kaggle ([link](https://www.kaggle.com/yamqwe/chicago-crimee?select=chicago_crime_2014.csv))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvhhltq3Zm5-"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/andre-marcos-perez/ebac-course-utils/main/dataset/crime.csv -q -O crime.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVSxCr2VZmD0"
      },
      "source": [
        "Vamos criar um DataFrame Pandas com os dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pIIqjB0Ab1j"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "filename = './crime'\n",
        "\n",
        "df = pd.read_csv(f'./{filename}.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdZ6pagbZ0hV"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdC-IMZLrUGp"
      },
      "source": [
        "Vamos então conferir alguns metadados do DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lt3J8_Q8qNlL"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqomksa6Pvfb"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2Thqn0jrWbr"
      },
      "source": [
        "Por fim, vamos realizar uma agregação para futura comparação. Nela, vamos contar a frequencia de ocorrência dos crimes agrupados localidades da cidade (coluna `Location Description`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGviw8b3sPUu"
      },
      "outputs": [],
      "source": [
        "agg_df = df['Location Description'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWftDL9jsqxh"
      },
      "outputs": [],
      "source": [
        "agg_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhKOYuGqLMSo"
      },
      "source": [
        "### **2.1. Apache Parquet**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvHMHCjZQHEm"
      },
      "source": [
        "O `Apache Parquet` é o formato de arquivo **orientado a coluna** mais utilizado no ecossistema de *big data* ([documentação](https://parquet.apache.org/)). Entre suas funcionalidades, podemos destacar:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaiZiLLhF23E"
      },
      "source": [
        "  - indexação por coluna (processamento);\n",
        "  - tipagem por coluna (processamento e armazenamento);\n",
        "  - compressão por coluna (armazenamento)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAwgjQOxF3mc"
      },
      "source": [
        "A interoperabilidade com o pacote Python `Pandas` é alcançada atrás do uso de estruturas de dados **orientadas a coluna**, como o `Apache Arrow`. Exemplos:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TkFYXMtGh0x"
      },
      "source": [
        " - Salvar um `Pandas` DataFrame para um arquivo `Apache Parquet`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOFXezq7QFeu"
      },
      "outputs": [],
      "source": [
        "df.to_parquet('./crime.parquet', engine='pyarrow')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB69_3ytRrmp"
      },
      "source": [
        " - Salvar um `Pandas` DataFrame para um arquivo `Apache Parquet` comprimido:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3l85cqhQ9-n"
      },
      "outputs": [],
      "source": [
        "df.to_parquet('./crime.parquet.gzip', engine='pyarrow', compression='gzip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMRVTkvnRtVN"
      },
      "source": [
        "Vamos utilizar o método `getsize` do pacote nativo `os` para estimar o tamanho dos arquivos na memória persistente (ROM/SSD):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtiQ431gQoco"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "extensions = ['csv', 'parquet', 'parquet.gzip']\n",
        "\n",
        "for extension in extensions:\n",
        "\n",
        "  size = os.path.getsize(f'{filename}.{extension}')\n",
        "  size_mb = round(size / 1024 / 1024, 2)\n",
        "\n",
        "  print(f'{extension}: {size_mb} MB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiCZBGWWYVO3"
      },
      "source": [
        "### **2.2. Apache Arrow**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaZMlCB7YXqx"
      },
      "source": [
        "O `Apache Arrow` é o uma estrutura de dados **orientado a coluna** muito utilizada no ecossistema de *big data* ([documentação](https://arrow.apache.org/)). É equivalente ao `Apache Parquet`, mas em memória, como listas, dicionários e objetos Python. O pacote Python `PyArrow` ([documentação](https://arrow.apache.org/docs/python/install.html)) permite a criação e manipulação das estruturas de dados do `Apache Arrow`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAT98R6YephH"
      },
      "outputs": [],
      "source": [
        "!pip install pyarrow==7.0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwW46ZsAhQbD"
      },
      "source": [
        "O `PyArrow` trabalha com uma estrutura de dados orientada a coluna conhecida como `table` (tabela), similar aos DataFrames `Pandas`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBZK5zlyYX1c"
      },
      "outputs": [],
      "source": [
        "from pyarrow import csv\n",
        "import pyarrow as pa\n",
        "import pandas as pd\n",
        "\n",
        "filename = './crime'\n",
        "\n",
        "table = csv.read_csv(f'{filename}.csv')\n",
        "df = pd.read_csv(f'./{filename}.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cxyb2KkCqlHX"
      },
      "outputs": [],
      "source": [
        "table.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nxso4vVceLfb"
      },
      "outputs": [],
      "source": [
        "table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-ecMtXljQHV"
      },
      "source": [
        "A similaridade com o Pandas fica evidente quando realizamos operações de agregação."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xje6utUqI6Ib"
      },
      "source": [
        " - Pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jyzjkvr3I0hp"
      },
      "outputs": [],
      "source": [
        "agg_df = df['Location Description'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxvVaBgiI2jB"
      },
      "outputs": [],
      "source": [
        "agg_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzDMbplAI4Oj"
      },
      "source": [
        " - PyArrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMMsgFnviVvK"
      },
      "outputs": [],
      "source": [
        "agg_table = table.group_by('Location Description').aggregate([('Location Description', 'count')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BvPcbQ6j80n"
      },
      "outputs": [],
      "source": [
        "agg_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWqI4rUWmXnb"
      },
      "source": [
        "Vamos utilizar o método `getsizeof` do pacote nativo `sys` para estimar o tamanho dos objetos na memória de trabalho (RAM):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85SNQ-MvmX6-"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "objects = [{'pandas': df}, {'pyarrow': table}]\n",
        "\n",
        "for obj_dict in objects:\n",
        "  for id, obj in obj_dict.items():\n",
        "    \n",
        "    size = sys.getsizeof(obj)\n",
        "    size_mb = round(size / 1024 / 1024, 2)\n",
        "\n",
        "    print(f'{id}: {size_mb} MB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RC9nVOOQtQQi"
      },
      "source": [
        "Vemos que o objeto gerado pelo `PyArrow` (`table`) é aproximadamente 3 vezes menor que o objeto (`dataframe`) utilizado `Pandas`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVejInuPakz5"
      },
      "source": [
        "## 3\\. Particionamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POlvHFptrQFE"
      },
      "source": [
        "Para obsevar os benefícios que o **particionamento** trás para o armazenamento de grandes volumes de dados, vamos explorar as técnicas de particionamento na *cloud* da AWS, utilizando os serviços AWS S3 e AWS Athena, e o seu efeito combinado com a **orientação a coluna** através do `Apache Parquet`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNz6f1-6mDB0"
      },
      "source": [
        "### **3.1. Dados**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTywnGJvAJ5r"
      },
      "source": [
        "Vamos criar a coluna `reference_date` a partir da coluna `Date` no formato YYYY-MM-DD e entender se ela será uma boa coluna de partião."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMDjNrzCrR-Y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "filename = './crime'\n",
        "\n",
        "df = pd.read_csv(f'./{filename}.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGweKfDKjXmR"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCMnpj5ZjmX0"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "df['reference_date'] = df['Date'].apply(lambda date: datetime.strptime(date.split(sep=' ')[0], '%m/%d/%Y').strftime('%Y-%m-%d'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRbBtBs7JtGM"
      },
      "outputs": [],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vGAKmCvo-lA"
      },
      "source": [
        "Uma coluna que separa os dados em grupos bem distribuidos é uma boa candidata a uma coluna de partição. Vamos contar as ocorrências de crimes (logo, linhas) em cada um dos dias da coluna `reference_date` recém criada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wcfdAuZio-_O"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m agg_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreference_date\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts())\u001b[38;5;241m.\u001b[39msort_index()\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m      2\u001b[0m agg_df \u001b[38;5;241m=\u001b[39m agg_df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreference_date\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamount\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m      3\u001b[0m agg_df \u001b[38;5;241m=\u001b[39m agg_df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreference_date\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
            "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "agg_df = pd.DataFrame(df['reference_date'].value_counts()).sort_index().reset_index()\n",
        "agg_df = agg_df.rename(columns={'reference_date': 'amount'})\n",
        "agg_df = agg_df.rename(columns={'index': 'reference_date'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v01g2HUIqTFY"
      },
      "outputs": [],
      "source": [
        "agg_df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFyq1FtfkctQ"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "with sns.axes_style('whitegrid'):\n",
        "\n",
        "  chart = sns.barplot(x='reference_date', y='amount', data=agg_df)\n",
        "  chart.set(xticklabels=[])\n",
        "  chart.set(title='Frequency of Crime per Day (Chicago, 2014)', xlabel='Date (values ommited)', ylabel='Absolute Frequency');\n",
        "  chart.figure.set_size_inches(w=40/2.54, h=15/2.54)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEXaqKv4MH4i"
      },
      "source": [
        "Observa-se que a coluna `reference_date` de fato divide os dados em grupos equilibrados. Sendo assim, vamos salvar o `DataFrame` Pandas em arquivos comprimidos no formato `Parquet`, particionados pela coluna `reference_date`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99CJIbkpLC64"
      },
      "outputs": [],
      "source": [
        "df.to_parquet('./crime', engine='pyarrow', compression='gzip', partition_cols='reference_date')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sizurY8Rty5"
      },
      "source": [
        "Vamos também salver o `DataFrame` Pandas no formato `CSV` para garantir que os arquivos de ambas as abordagens possuem a coluna `reference_date`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjcPDnVnRt9s"
      },
      "outputs": [],
      "source": [
        "df.to_csv('./crime_enriched.csv', sep=',', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87VyfhxJJivU"
      },
      "source": [
        "### **3.2. AWS S3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWj5ljVDLDsr"
      },
      "source": [
        "Na AWS, vamos criar os recursos tanto para o arquivo no formato `csv` quanto para os arquivos no formato `parquet`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gElLw7ULNsx2"
      },
      "source": [
        " - **CSV**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkTsE_hlRkRM"
      },
      "source": [
        "Vamos criar os recursos na AWS:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G27J4pgVRkRN"
      },
      "source": [
        "1. `Bucket` no `AWS S3` para armazenar o arquivo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgsdR1cyNvoU"
      },
      "source": [
        " - **Parquet**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdeAengbOne3"
      },
      "source": [
        "Vamos começar criando os recursos na AWS:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoYiyC2uPTP9"
      },
      "source": [
        "1. `Bucket` no `AWS S3` para armazenar os arquivos e suas partições;\n",
        "1. Usuário no `AWS IAM` para fazer o *upload* dos arquivos e suas partições."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBH2mGlhOFWR"
      },
      "source": [
        "Então, vamos inserir as credenciais no Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZhq1Z3iMML8"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "\n",
        "aws_access_key_id = getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3udXTzFMPSV"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "\n",
        "aws_secret_access_key = getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z77LJUMKN_Va"
      },
      "source": [
        "E instalar o pacote Boto3, o SDK Python da AWS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQXiK5nKMJKA"
      },
      "outputs": [],
      "source": [
        "!pip install boto3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw4zV7xnOkmF"
      },
      "source": [
        "Por fim, vamos criar o nosso cliente e fazer o *upload* das partições."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aD72G2s-MR7K"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "\n",
        "client = boto3.client(\n",
        "  's3',\n",
        "  aws_access_key_id=aws_access_key_id,\n",
        "  aws_secret_access_key=aws_secret_access_key\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ox1qvbcbMcHD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "BUCKET = 'modulo-42-ebac-parquet'\n",
        "\n",
        "i = 0\n",
        "\n",
        "for root, dirs, files in os.walk('./crime'):\n",
        "  elapsed = f'{round(100*i/365, 2)} %'\n",
        "  print(elapsed)\n",
        "  for file in files:\n",
        "    path = os.path.join(root, file)\n",
        "    bucket_path = '/'.join(path.split(sep='/')[2:])\n",
        "    client.upload_file(path, BUCKET, bucket_path)\n",
        "  i = i + 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGD63S7KreZf"
      },
      "source": [
        "### **3.3. AWS Athena**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NclK_W44kdxQ"
      },
      "source": [
        "Na AWS, vamos criar os recursos tanto para o arquivo no formato `csv` quanto para os arquivos no formato `parquet`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d4PmPKId2O7"
      },
      "source": [
        " - **CSV**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UcvQqeRd2PE"
      },
      "source": [
        "Vamos criar os recursos na AWS:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVC5mDdad2PE"
      },
      "source": [
        "1. Tabela no `AWS Athena` apontando para o arquivo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9WoKyQ7gAc2"
      },
      "source": [
        "```sql\n",
        "CREATE EXTERNAL TABLE `crime_csv`(\n",
        "  `index` bigint, \n",
        "  `id` string, \n",
        "  `case number` string, \n",
        "  `date` string, \n",
        "  `block` string, \n",
        "  `iucr` string, \n",
        "  `primary type` string, \n",
        "  `description` string, \n",
        "  `location description` string, \n",
        "  `arrest` string, \n",
        "  `domestic` string, \n",
        "  `beat` string, \n",
        "  `district` string, \n",
        "  `ward` string, \n",
        "  `community area` string, \n",
        "  `fbi code` string, \n",
        "  `latitude` string, \n",
        "  `longitude` string,\n",
        "  `reference_date` string)\n",
        "ROW FORMAT SERDE \n",
        "  'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
        " WITH SERDEPROPERTIES (\n",
        "   'separatorChar' = ',',\n",
        "   'quoteChar' = '\\\"',\n",
        "   'escapeChar' = '\\\\'\n",
        "   )\n",
        "STORED AS INPUTFORMAT \n",
        "  'org.apache.hadoop.mapred.TextInputFormat' \n",
        "OUTPUTFORMAT \n",
        "  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
        "LOCATION\n",
        "  's3://modulo-42-ebac-csv/'\n",
        "TBLPROPERTIES (\n",
        "  \"skip.header.line.count\"=\"1\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-nywk8Ad2PE"
      },
      "source": [
        " - **Parquet**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SatlYFEKd2PE"
      },
      "source": [
        "Vamos criar os recursos na AWS:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7OeaaoPd78a"
      },
      "source": [
        "1. Tabela no `AWS Athena` apontando para os arquivos e suas partições.\n",
        "1. Carregar as partições."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmKucNY7fnZI"
      },
      "source": [
        "```sql\n",
        "CREATE EXTERNAL TABLE `crime_parquet`(\n",
        "  `index` bigint, \n",
        "  `id` bigint, \n",
        "  `case number` string, \n",
        "  `date` string, \n",
        "  `block` string, \n",
        "  `iucr` string, \n",
        "  `primary type` string, \n",
        "  `description` string, \n",
        "  `location description` string, \n",
        "  `arrest` boolean, \n",
        "  `domestic` boolean, \n",
        "  `beat` bigint, \n",
        "  `district` bigint, \n",
        "  `ward` double, \n",
        "  `community area` double, \n",
        "  `fbi code` string, \n",
        "  `latitude` double, \n",
        "  `longitude` double)\n",
        "PARTITIONED BY ( \n",
        "  `reference_date` string)\n",
        "ROW FORMAT SERDE \n",
        "  'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' \n",
        "STORED AS INPUTFORMAT \n",
        "  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' \n",
        "OUTPUTFORMAT \n",
        "  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n",
        "LOCATION\n",
        "  's3://modulo-42-ebac-parquet/'\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53p9CiWDkRh_"
      },
      "source": [
        "```sql\n",
        "MSCK REPAIR TABLE `crime_parquet`;\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhmDaLBfkgR0"
      },
      "source": [
        "Por fim, vamos executar um conjunto de consultas SQL em ambas as tabelas e observar a quantidade de dados escaneados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHv7bSnxmb2n"
      },
      "source": [
        " - Efeito da **orientação a coluna**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7u2_angjmL9O"
      },
      "source": [
        "```sql\n",
        "SELECT \"location description\", COUNT(1) as \"amount\"\n",
        "FROM crime_csv\n",
        "GROUP BY 1\n",
        "ORDER BY 2 DESC;\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lus8jJtJmPmF"
      },
      "source": [
        "```sql\n",
        "SELECT \"location description\", COUNT(1) as \"amount\"\n",
        "FROM crime_parquet\n",
        "GROUP BY 1\n",
        "ORDER BY 2 DESC;\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrEu9JgqmtVj"
      },
      "source": [
        "A consulta escaneou 47.34 MB para a tabela `crime_csv`, que é o mesmo tamanho do arquivo, logo um *full scan*. Já para a tabeka `crime_parquet`, a consulta escaneou 0.44 MB. Ou seja, a tabela com o dado **orientado a coluna** escaneou **108 vezes** menos dados para a consulta SQL que seu par em `csv`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAEhG6_cnuls"
      },
      "source": [
        " - Efeito do **particionamento**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeaDP0eQn6qH"
      },
      "source": [
        "```sql\n",
        "SELECT *\n",
        "FROM crime_csv\n",
        "WHERE CAST(reference_date as DATE) BETWEEN DATE '2014-12-01' and DATE '2014-12-31'\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXgHc5iVn6qI"
      },
      "source": [
        "```sql\n",
        "SELECT *\n",
        "FROM crime_parquet\n",
        "WHERE CAST(reference_date as DATE) BETWEEN DATE '2014-12-01' and DATE '2014-12-31'\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpjcsoOIn6qI"
      },
      "source": [
        "A consulta escaneou 47.34 MB para a tabela `crime_csv`, que é o mesmo tamanho do arquivo, logo um *full scan*. Já para a tabeka `crime_parquet`, a consulta escaneou 1.00 MB. Ou seja, a tabela com o dado **particionado** escaneou **47.34 vezes** menos dados para a consulta SQL que seu par em `csv`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GimKNOuorQD"
      },
      "source": [
        " - Efeito da **orientação a coluna** e do **particionamento**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIeMM80porQD"
      },
      "source": [
        "```sql\n",
        "SELECT \"location description\", COUNT(1) as \"amount\"\n",
        "FROM crime_csv\n",
        "WHERE CAST(reference_date as DATE) BETWEEN DATE '2014-12-01' and DATE '2014-12-31'\n",
        "GROUP BY 1\n",
        "ORDER BY 2 DESC\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGHWrAMSorQE"
      },
      "source": [
        "```sql\n",
        "SELECT \"location description\", COUNT(1) as \"amount\"\n",
        "FROM crime_parquet\n",
        "WHERE CAST(reference_date as DATE) BETWEEN DATE '2014-12-01' and DATE '2014-12-31'\n",
        "GROUP BY 1\n",
        "ORDER BY 2 DESC\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksiGclEvorQE"
      },
      "source": [
        "A consulta escaneou 47.34 MB para a tabela `crime_csv`, que é o mesmo tamanho do arquivo, logo um *full scan*. Já para a tabeka `crime_parquet`, a consulta escaneou 0.04 MB. Ou seja, a tabela com o dado **particionado** e **orientado a coluna** escaneou **1183.5 vezes** menos dados para a consulta SQL que seu par em `csv`. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "modulo_42_aula.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
