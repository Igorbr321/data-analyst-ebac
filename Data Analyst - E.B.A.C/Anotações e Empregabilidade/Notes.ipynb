{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f60c87bb",
   "metadata": {},
   "source": [
    "# Programa de Formação - Analista de Dados\n",
    "\n",
    "## Módulos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bd58b5",
   "metadata": {},
   "source": [
    "\n",
    "### **Python**\n",
    "- **Introdução ao Python e Jupyter Notebook**: Familiarização com a interface Jupyter Notebook; execução de células; primeiros passos com código em Python.\n",
    "- **Tipos de Dados, Variáveis e Operações Básicas**: Declaração e uso de variáveis; tipos de dados como `int`, `float`, `string`, `bool`; operadores aritméticos e lógicos.\n",
    "- **Estruturas Condicionais e de Repetição**: Uso de `if`, `elif`, `else`; loops `for` e `while`; controle de fluxo e estruturas de decisão.\n",
    "- **Funções, Métodos e Escopo**: Definição de funções com `def`; parâmetros, argumentos, retorno de valores; escopo de variáveis (local e global).\n",
    "- **Manipulação de Listas, Dicionários e Conjuntos**: Métodos e operações em listas, dicionários e conjuntos; compreensão de listas; iterações e manipulações de dados complexos.\n",
    "- **Manipulação de Dados com Pandas**: Criação e manipulação de `DataFrames` e `Series`; importação e exportação de dados; filtragem, agregação e transformação de dados.\n",
    "- **Visualização de Dados com Matplotlib e Seaborn**: Criação de gráficos básicos (barras, linhas, histogramas); personalização de gráficos; gráficos avançados com Seaborn (heatmaps, pairplots).\n",
    "- **Tratamento de Dados e Limpeza**: Remoção de valores ausentes, tratamento de duplicados; transformação de colunas; tratamento de outliers e dados inconsistentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41045378",
   "metadata": {},
   "source": [
    "### **Análise de Dados**\n",
    "\n",
    "- **Coleta de Dados com Web Scraping:** Introdução ao web scraping; uso de bibliotecas como `BeautifulSoup` e `requests`; extração de dados de sites e criação de datasets\n",
    "- **Data Wrangling (Tratamento e Limpeza de Dados):** Processamento de dados para limpeza e estruturação; tratamento de valores ausentes, duplicados e outliers; transformação e organização dos dados para análise.\n",
    "- **Visualização de Dados:** Criação de gráficos e visualizações com Matplotlib e Seaborn; escolha de visualizações apropriadas para diferentes tipos de dados; customização e interpretação de gráficos.\n",
    "> - **Projeto de Análise Exploratória de Dados (EDA):** Realização de uma análise exploratória completa; identificação de padrões, tendências e insights; estruturação e apresentação dos resultados em relatórios.\n",
    "- **Controle de Versão com Git:** Conceitos de Git e GitHub; controle de versões e colaboração; criação de repositórios, commits, branches e merge.\n",
    "- **Fundamentos de Matemática:** Trabalhando com arrays com a biblioteca Numpy.\n",
    "- **Fundamentos de Estatística:** Estudando como calcular a média e o desvio padrão, além de aprender a ajustar os dados (padronizar) para que os modelos de aprendizado de máquina funcionem melhor.  \n",
    "- **Aprendizado de Máquina - Fundamentos:** Entendendo técnicas básicas de preparação de dados, como identificar a variável de saída (resposta), organizar os dados que serão usados para fazer previsões e classificar colunas em categorias, como valores sem ordem (nominal) e com ordem (ordinal).\n",
    "- Aprendizado Supervisionado\n",
    "    - **Aprendizado de Máquina - Regressão:** Entendendo os conceitos de regressão linear, classificando categorias (nominal e ordinal), ajustando colunas numéricas com valores muito diferentes (padronização), usando bibliotecas do sklearn para treinar o modelo e avaliando sua qualidade.\n",
    "        - **Regressão:** Responde perguntas como \"qual é o valor?\"\n",
    "    - **Aprendizado de Máquina - Classificação:** Entendendo os conceitos de classificação, realizando o mesmo padrão de categorizar e padronizar, porém, a diferença \n",
    "        - **Classificação:** Responde perguntas como \"qual é a categoria?\"\n",
    "    - **Aprendizado de Máquina - Séries Temporais:** Abrange técnicas essenciais para analisar dados temporais, incluindo slicing e resampling para manipular intervalos e frequências de dados. Inclui também métodos para identificar e isolar componentes fundamentais, como tendência (padrões de longo prazo), sazonalidade (flutuações periódicas) e resíduo (variações imprevisíveis). Essas técnicas permitem compreender e modelar padrões temporais, fornecendo insights para previsões e análises de comportamento ao longo do tempo.\n",
    "- Aprendizado Não Supervisionado\n",
    "    - **Aprendizado de Máquina - Agrupamento:** Os dados são organizados em grupos semelhantes chamados clusters. O algoritmo K-means, um dos métodos mais usados, define pontos centrais, ou centróides, e associa cada dado ao centróide mais próximo. Para decidir o número ideal de clusters, usa-se o método do cotovelo, que analisa a soma das distâncias (WCSS) entre pontos e centróides, escolhendo o ponto onde a redução no WCSS começa a diminuir significativamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cff070",
   "metadata": {},
   "source": [
    "### **Análise de Dados para Inteligência de Negócios**\n",
    "\n",
    "- **Conhecendo o Google Data Sutdio(Looker)**: Montando Dashboards com o Looker(criação de dashboards online), conhecendo os KPI´s(key performance indicators) e fazendo EDA(exploratoy data analysis).\n",
    "<br><br>\n",
    "> - **Projeto Prático - Dashboard de Dados**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4add22bf",
   "metadata": {},
   "source": [
    "### **SQL**\n",
    "- **Introdução a Banco de Dados Relacionais**: Estrutura e organização de bancos de dados; conceitos de tabelas, linhas e colunas; normalização e integridade de dados.\n",
    "- **Comandos Básicos: SELECT, INSERT, UPDATE, DELETE**: Comandos essenciais para manipulação de dados; inserção e modificação de registros; exclusão e recuperação de dados específicos.\n",
    "- **Filtragem de Dados com WHERE, LIKE e BETWEEN**: Filtragem avançada de dados com condições; uso de curingas com `LIKE`; filtragem de intervalos com `BETWEEN`.\n",
    "- **Agregação e Funções de Agregação (SUM, AVG, COUNT)**: Cálculos de agregação para sumarização de dados; uso de funções para totalizações, médias e contagens em grupos.\n",
    "- **JOINs: INNER, LEFT, RIGHT, FULL**: Conexão de tabelas com JOINs; combinação de dados em múltiplas tabelas; tipos de JOINs e quando usá-los.\n",
    "- **Subqueries e VIEWS**: Uso de subconsultas para filtragem avançada; criação e uso de VIEWS para consultas reutilizáveis e segurança de dados.\n",
    "- **Indexação e Otimização de Consultas**: Melhorias de desempenho com índices; práticas para otimizar consultas e reduzir tempo de resposta.\n",
    "<br><br>\n",
    "> - **Projeto Prático: Criação de um Banco de Dados Relacional para Análise**: Projeto integrando criação de tabelas, inserção de dados e execução de consultas analíticas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4805aec5",
   "metadata": {},
   "source": [
    "### **Computação em Nuvem e Big Data**\n",
    "- **Introdução à Computação em Nuvem e Big Data**: Conceitos de computação em nuvem e suas aplicações; vantagens de armazenamento e processamento distribuído.\n",
    "- **Conceitos Básicos de AWS e Serviços Essenciais**: Introdução aos serviços da AWS; navegação no console AWS; uso de S3 para armazenamento e Athena para análise de dados.\n",
    "- **Armazenamento com S3 e Análise de Dados com Athena**: Criação e gerenciamento de buckets S3 para armazenar grandes volumes de dados; execução de consultas SQL com AWS Athena diretamente no S3.\n",
    "- **Automação de Fluxos de Trabalho**: Configuração de eventos e automação com AWS EventBridge e Step Functions para orquestração de processos de dados; controle de acesso com IAM.\n",
    "- **API Gateway e AWS Lambda para Processamento Sob Demanda**: Criação de endpoints com API Gateway e uso de AWS Lambda para execução de funções sob demanda, respondendo a eventos específicos.\n",
    "- **Uso do PySpark, Parquet e PyArrow para Manipulação de Dados em Massa**: Manipulação e transformação de grandes volumes de dados com DataFrames e RDDs no PySpark; uso de formatos de arquivo Parquet e PyArrow para otimização e armazenamento eficiente.\n",
    "- **Processamento Distribuído e Clusters**: Conceitos de clusters e nós; escalabilidade e gerenciamento de recursos em ambientes de cluster.\n",
    "- **Estrutura de Dados e Consultas em Big Data**: Análise e otimização de consultas para grandes datasets; modelagem e armazenamento eficiente de dados.\n",
    "<br><br>\n",
    "- **Projeto Final - Pipeline - Telegram**\n",
    "    - Utilizando as ferramentas AWS (S3, Athena, Lambda, API Gateway, IAM, EventBridge) e um bot criado no Telegram, coletamos os dados que são gerados pelo usuário, salvamos em dois buckets(raw e enriched) um somente para armazenamento dos dados <br>\n",
    "    sem tratamento e um dos dados tratados;\n",
    "    - Utilizando Athena, podemos fazer EDA e criar dashboards para apresentar o projeto.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
